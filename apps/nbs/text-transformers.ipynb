{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy   as np \n",
    "import pandas  as pd \n",
    "\n",
    "# #!pip install -U pandera\n",
    "# #!pip install -U transformers\n",
    "# #!pip install -U datasets\n",
    "# import pandera as pa \n",
    "# from   transformers import pipeline, DataCollatorWithPadding\n",
    "# from   transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "# from   datasets import load_dataset, load_metric, Dataset \n",
    "# #from  deepchecks.tabular import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keebler \n",
    "# from keebler.data.utils import trsfrm_dt\n",
    "# from keebler.core.io.utils import trsfrm_to_camel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBase(object):\n",
    "    def __init__(self, class_map:dict, model_name:str):\n",
    "        super().__init__()\n",
    "        bert_tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        data_collator  = DataCollatorWithPadding(tokenizer=bert_tokenizer)\n",
    "        # load pretrained model with edicated class map\n",
    "        num_labels     = len(class_map) \n",
    "        self.seq_clf_model  = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.seq_clf_model.config.id2label = class_map\n",
    "\n",
    "class ModelStore(object):\n",
    "    def __init__(self, artifact:str):\n",
    "        artifact.push_to_hub(\n",
    "            \"profoz/covid\", \n",
    "            use_auth_token=api_key,\n",
    "            use_temp_dir=True,\n",
    "            revision='1'\n",
    "        )\n",
    "\n",
    "bert_tokenizer.push_to_hub(\n",
    "    \"profoz/covid\",\n",
    "    use_auth_token=api_key,\n",
    "    use_temp_dir=True,\n",
    "    revision='1'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "class_map  = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "model      = ModelBase(class_map, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'train': \"/workspaces/keebler-studio/data/chemical-formula/CNH_033_full.csv\"}\n",
    "dataset   = load_dataset(\"csv\", data_files=data_dict, delimiter=\",\", split='train')\n",
    "metrics   = dict(num_rows=dataset.num_rows, num_columns=dataset.num_columns)\n",
    "df        = Dataset.to_pandas(dataset).drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "df_schema_test = pd.DataFrame({\n",
    "    \"int_col\":   range(100),\n",
    "    \"float_col\": map(float, reversed(range(100))),\n",
    "    \"str_col\":   [\"hi\"] * 100\n",
    "})\n",
    "\n",
    "# object based API: Simple Schema\n",
    "schema_simple = pa.DataFrameSchema( {\n",
    "    \"int_col\":   pa.Column(int),\n",
    "    \"float_col\": pa.Column(float),\n",
    "    \"str_col\":   pa.Column(str)\n",
    "})\n",
    "# object based API: Check based APi to express validation rules\n",
    "schema_check = pa.DataFrameSchema({\n",
    "    \"int_col\":   pa.Column(int,   pa.Check.ge(0)),      # pa.Check(lambda Series: series >= 0), pa.Check(lambda Series: series.mean() > 0)\n",
    "    \"float_col\": pa.Column(float, pa.Check.lt(101)),\n",
    "    \"str_col\":   pa.Column(str,   pa.Check.eq(\"hi\"))\n",
    "})\n",
    "#class-based API: pydantic style \n",
    "class Schema(pa.SchemaModel):\n",
    "    int_col:pa.typing.Series[int]     = pa.Field(ge=0)\n",
    "    float_col:pa.typing.Series[float] = pa.Field(lt=101)\n",
    "    str_col:pa.typing.Series[str]     = pa.Field(eq=\"hi\")\n",
    "    index:pa.typing.Index[int]\n",
    "\n",
    "# Schema.to_schema()\n",
    "# custom check, like a UDF implementation rather than inline \n",
    "@pa.check_types(lazy=True)\n",
    "def test_schema(df: pa.typing.DataFrame[Schema]) -> pa.typing.DataFrame[Schema]:\n",
    "    return df \n",
    "# @pa.check \n",
    "# def custom_check(cls, series:pd.Series) -> pd.Series:\n",
    "#     return series >= 0 \n",
    "df_schema_test.loc[:5, \"int_col\"] = -1\n",
    "print( test_schema(df_schema_test) )\n",
    "\n",
    "\n",
    "# # if it passes the dataframe checks, if should just pass a dataframe\n",
    "# # error returns index and failure case; failure case values, condition that failed \n",
    "# # further on corruption of dataframe, if will error out immediately (SchemaErrorÃŸ)\n",
    "# try: \n",
    "#     report = schema_simple(df_schema_test, lazy=True)\n",
    "# except pa.errors.SchemaErrors as exc:\n",
    "#     failure_cases = exc.failure_cases\n",
    "#     print(failure_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: ml models built to be goal seeking, but goal needs to be in terms of business outcome\n",
    "# - if you are not clear about the goal, you are not building the right stuff\n",
    "\n",
    "# Text (pre-processed) into a format the model can understand\n",
    "# - SuperAgent to collect and sample more representative samples\n",
    "# Pre-Processed: passed to the model\n",
    "# Post-Processed: predictions are post-processed so you can make sense of time\n",
    "\n",
    "# Measurement\n",
    "# - If we don't know how to measure it, we don't build any solution\n",
    "# - What's the definition of good, why are we building this model\n",
    "\n",
    "# Monitoring\n",
    "# Human and Machine Partnership\n",
    "# - Person involved in understanding and monitoring the models   \n",
    "# Testing: Unintended Consequences, exposed to the risks, companion diagnostics, esnure integrity of the AI model\n",
    "# - Risk Level Asessment across Use Cases *****\n",
    "# - Risk Level on data and knowledge (e.g., privacy)\n",
    "# - High Cost Curve\n",
    "# Integrity: Design a solution natively with integrity in mind\n",
    "# - models should not fail on missing features\n",
    "# - identify bias: data is biased, sampling is biased. \n",
    "# - measurment: take first step to measure (historically, where to be, set targets where want to be).\n",
    "# - measurement: how to measure diversity\n",
    "# - bias, measurment, understanding, and shaping to ensure no negative outcomes that are inappropriate.\n",
    "# - corrective action: corrective action\n",
    "\n",
    "\n",
    "\n",
    "def exec_pipeline(text:str, name:str='ner'):\n",
    "    \"\"\"Execute a Pipeline\n",
    "    \n",
    "    >>> ner = pipeline(\"ner\", grouped_entities=True)\n",
    "    >>> ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n",
    "\n",
    "    >>> cls = pipeline(\"zero-shot-classification\")\n",
    "    cls(\n",
    "        \"This is a course about the Transformers library\",\n",
    "        candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    "    )\n",
    "\n",
    "    >>> cls = pipeline(\"sentiment-analysis\")\n",
    "    >>> cls(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "    >>> cls([\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"])\n",
    "\n",
    "    >>> ner = pipeline(\"ner\", grouped_entities=True)\n",
    "    >>> ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n",
    "\n",
    "    \"\"\"\n",
    "    pass \n",
    "\n",
    "# springboard, innovation risk\n",
    "# speed and innovation, prepared for the worst case\n",
    "# assurance to business: reptuation risk is managed (when adopting AI), nothing bad will happen, impact on revenue\n",
    "# how to help me to grow\n",
    "# Cost of being wrong: which is more controlled vs live and die (higher stakes), which ones are tolerable\n",
    "# OKRs\n",
    "\n",
    "df = df.map(\n",
    "    lambda x: {'text_len': len(x['comments'].split())}\n",
    ").filter(\n",
    "    lambda x: x['text_len' > 2]\n",
    ")\n",
    "df = df.map(\n",
    "    lambda x: {'text': \"\".join([x['comments'],\"/n\"]) + \n",
    "                       \"\".join([x['title'],\"/n\"])  }\n",
    ")\n",
    "\n",
    "\n",
    "#metadata = len(ds.column_names), ds.version, ds.citation\n",
    "#df = df.shuffle(seed=42).select(range(1000))\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac6dcd399ab8cd2ecc0f6e8f154efd75b4aa37c8093b73598011b1aae01f02fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
